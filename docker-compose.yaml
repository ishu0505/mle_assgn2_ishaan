# This docker-compose file uses a stable SQLite setup for local development.
services:
  # This single init service reliably prepares the database and creates the admin user.
  airflow-init:
    build: .
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      # FIX: Add the project root to the PYTHONPATH. This makes modules like 'configs' and 'utils' importable.
      - PYTHONPATH=/opt/airflow
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./configs:/opt/airflow/configs
      - ./utils:/opt/airflow/utils
      - ./data:/opt/airflow/data
      - ./datamart:/opt/airflow/datamart
      - ./model_store:/opt/airflow/model_store
      - airflow_data:/opt/airflow
    entrypoint: >
      bash -c "
        airflow db upgrade &&
        airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true
      "

  airflow-webserver:
    build: .
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__FERNET_KEY=PAqBeGJLJTYFzVkOGHWIYXdLO7XdXz5yTdxAGJe9ezM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      # FIX: Add the project root to the PYTHONPATH.
      - PYTHONPATH=/opt/airflow
      # --- PERFORMANCE TUNING ---
      - AIRFLOW__CORE__PARALLELISM=10
      - AIRFLOW__CORE__DAG_CONCURRENCY=8
      - AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=4
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./configs:/opt/airflow/configs
      - ./utils:/opt/airflow/utils
      - ./data:/opt/airflow/data
      - ./datamart:/opt/airflow/datamart
      - ./model_store:/opt/airflow/model_store
      - airflow_data:/opt/airflow
    ports:
      - "8080:8080"
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "curl --fail http://localhost:8080/health"]
      interval: 10s
      timeout: 10s
      retries: 5

  airflow-scheduler:
    build: .
    depends_on:
      airflow-init:
        condition: service_completed_successfully
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
      - AIRFLOW__CORE__FERNET_KEY=PAqBeGJLJTYFzVkOGHWIYXdLO7XdXz5yTdxAGJe9ezM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      # FIX: Add the project root to the PYTHONPATH.
      - PYTHONPATH=/opt/airflow
      # --- PERFORMANCE TUNING ---
      - AIRFLOW__CORE__PARALLELISM=10
      - AIRFLOW__CORE__DAG_CONCURRENCY=8
      - AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG=4
    volumes:
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./configs:/opt/airflow/configs
      - ./utils:/opt/airflow/utils
      - ./data:/opt/airflow/data
      - ./datamart:/opt/airflow/datamart
      - ./model_store:/opt/airflow/model_store
      - airflow_data:/opt/airflow
    command: scheduler

volumes:
  airflow_data: {}
